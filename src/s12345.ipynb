{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "114a6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date  ave_flot_air_flow  ave_flot_level  iron_feed  \\\n",
      "0  2017-04-24 00:00:00         300.263166      383.982443      55.17   \n",
      "1  2017-04-24 01:00:00         299.782402      386.049069      55.17   \n",
      "2  2017-04-24 02:00:00         299.750052      385.250935      55.17   \n",
      "3  2017-04-24 03:00:00         299.997522      389.635519      55.17   \n",
      "4  2017-04-24 04:00:00         300.005220      387.810807      55.17   \n",
      "\n",
      "   starch_flow  amina_flow  ore_pulp_flow  ore_pulp_pH  ore_pulp_density  \\\n",
      "0  1979.589150  599.676489     400.017222     9.774028          1.753206   \n",
      "1  1758.466329  600.043100     400.484528     9.539246          1.754190   \n",
      "2  2379.752428  599.948406     400.325617     9.434227          1.756873   \n",
      "3  2287.130046  599.580383     399.801506     9.725607          1.727125   \n",
      "4  2291.789167  599.871217     399.567333     9.845198          1.633063   \n",
      "\n",
      "   silica_concentrate  \n",
      "0            4.360000  \n",
      "1            3.290000  \n",
      "2            4.900000  \n",
      "3            4.860153  \n",
      "4            4.780898  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1817 entries, 0 to 1816\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                1817 non-null   object \n",
      " 1   ave_flot_air_flow   1817 non-null   float64\n",
      " 2   ave_flot_level      1817 non-null   float64\n",
      " 3   iron_feed           1817 non-null   float64\n",
      " 4   starch_flow         1817 non-null   float64\n",
      " 5   amina_flow          1817 non-null   float64\n",
      " 6   ore_pulp_flow       1817 non-null   float64\n",
      " 7   ore_pulp_pH         1817 non-null   float64\n",
      " 8   ore_pulp_density    1817 non-null   float64\n",
      " 9   silica_concentrate  1817 non-null   float64\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 142.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv ( \"../data/raw_data/raw.csv\")\n",
    "\n",
    "print(df.head(5))\n",
    "print(df.info())\n",
    "y = df['silica_concentrate']\n",
    "X = df.drop(columns=['date','silica_concentrate'])      # On drop la date et la cible, la date n'est pas un paramÃ¨tre a traiter comme les autres si on devait le faire\n",
    "\n",
    "# Fait le decoupage XY et Train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "\n",
    "# sauve les dataframes dans le dossier data/processed\n",
    "X_train.to_csv('../data/processed_data/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed_data/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed_data/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed_data/y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8874eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = pd.read_csv('../data/processed_data/X_train.csv')\n",
    "X_test = pd.read_csv('../data/processed_data/X_test.csv')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)   # Fit le scaler sur les donnÃ©es d'entrainement et applique la transformation\n",
    "X_test_scaled = scaler.transform(X_test)         # Transform sans fit, pour eviter la fuite de donnÃ©e\n",
    "\n",
    "# Reconvertir en DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "X_train_scaled.to_csv('../data/processed_data/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed_data/X_test_scaled.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10f133dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramÃ¨tres trouvÃ©s : {'alpha': 16.5}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ðŸ”¹ Charger les data \n",
    "X_train = pd.read_csv('../data/processed_data/X_train_scaled.csv')\n",
    "y_train = pd.read_csv('../data/processed_data/y_train.csv').values.ravel()  # .ravel() pour Ã©viter une shape (n,1)\n",
    "\n",
    "# ðŸ”¹ DÃ©finir le modÃ¨le et la grille de params\n",
    "model = Ridge()\n",
    "param_grid = {\n",
    "    #'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "    'alpha': [5, 8, 10, 12, 14, 15, 15.5, 16, 16.5, 16.75, 17, 20, 25, 50, 75],  # Valeurs plus larges pour une meilleure exploration\n",
    "}\n",
    "\n",
    "# ðŸ”¹ GridSearch\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ðŸ”¹ Afficher les meilleurs params\n",
    "print(\"Meilleurs paramÃ¨tres trouvÃ©s :\", grid_search.best_params_)\n",
    "\n",
    "with open('../models/best_param.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search.best_params_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2482413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "import pickle\n",
    "\n",
    "with open('../models/best_param.pkl', 'rb') as f:\n",
    "    best_params = pickle.load(f)\n",
    "\n",
    "X_train_scaled = pd.read_csv('../data/processed_data/X_train_scaled.csv')\n",
    "y_train = pd.read_csv('../data/processed_data/y_train.csv').values.ravel()  # .ravel() pour Ã©viter une shape (n,1)\n",
    "\n",
    "model = Ridge(**best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "with open('../models/best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb017f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "read",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/best_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed_data/X_test_scaled.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m y_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed_data/y_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel()    \n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: read"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import json\n",
    "\n",
    "\n",
    "with open('../models/best_model.pkl', 'rb') as f:\n",
    "    model = pickle.load( f)\n",
    "\n",
    "X_test_scaled = pd.read_csv('../data/processed_data/X_test_scaled.csv')\n",
    "y_test = pd.read_csv('../data/processed_data/y_test.csv').values.ravel()    \n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"R2 : {r2}\")\n",
    "\n",
    "# Creer un dico avec les scores\n",
    "scores = {\n",
    "    \"mse\": mse,\n",
    "    \"r2\": r2\n",
    "}\n",
    "\n",
    "# Sauvegarder au format JSON\n",
    "with open('../metrics/scores.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
